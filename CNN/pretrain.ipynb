{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, math, json, random, numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "from seg_models_train import *\n",
    "from architecture import UNetBasic, UNetDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- config ----------------\n",
    "DATA_DIR   =  \"/home/cj535/palmer_scratch/TNG50_cutouts/MW_sample_maps/packed_aug8_weightemitvel\"  # folder with your .npy packs\n",
    "PATTERN    = \"TNG50_snap099_subid*_views10_aug8_C5_256x256.npy\"\n",
    "H, W = 256, 256\n",
    "R_MASK = 20                     # pixels\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 2e-4\n",
    "NUM_WORKERS = 1\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VEL_BINS = [(-300, -100), (-100, 100), (100, 300)]  # 3 input channels\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetDropout(in_channels=4,out_channels=4,p=0.2).to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scaler= torch.amp.GradScaler(enabled=(\"cuda\" in DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "packs = find_packs(DATA_DIR)\n",
    "all_subids = sorted(packs.keys(), key=lambda s: int(s))\n",
    "print(f\"Loaded {len(all_subids)} galaxies into RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) split by subid (no leakage)\n",
    "groups = np.array([int(s) for s in all_subids])\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "# Split operates on indices; use subids as both samples and groups\n",
    "idx = np.arange(len(all_subids))\n",
    "train_idx, test_idx = next(splitter.split(idx, groups=groups))\n",
    "train_subids = [all_subids[i] for i in train_idx]\n",
    "test_subids  = [all_subids[i] for i in test_idx]\n",
    "print(f\"Train galaxies: {len(train_subids)} | Test galaxies: {len(test_subids)}\")\n",
    "\n",
    "\n",
    "compression = 'log10'\n",
    "# 3) compute input normalization on train only\n",
    "mean, std = compute_input_norm(packs, train_subids,compression=compression)\n",
    "#mean, std = np.array([0,0,0]), np.array([1,1,1])*1e-20\n",
    "print(\"Input mean:\", mean, \"std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "los = 9\n",
    "flip = 0\n",
    "rot = 0\n",
    "n = los*8 + flip*4 + rot\n",
    "Hmid = packs[train_subids[i]][n][1]\n",
    "if compression == 'log10':\n",
    "    Hmid = np.log10(Hmid)\n",
    "elif compression == 'sqrt':\n",
    "    Hmid = np.sqrt(Hmid)\n",
    "\n",
    "#Hmid = (Hmid - mean[1])# / std[1]\n",
    "plt.imshow(Hmid,origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "vel_u = packs[train_subids[i]][n][4]\n",
    "vel_v = packs[train_subids[i]][n][5]\n",
    "plt.imshow(vel_v,origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) datasets / loaders\n",
    "train_ds = GalaxyPackDataset(packs, train_subids, mean=mean, std=std, r_mask=R_MASK,compression=compression)\n",
    "test_ds  = GalaxyPackDataset(packs, test_subids,  mean=mean, std=std, r_mask=R_MASK,compression=compression)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(\"cuda\" in DEVICE),\n",
    "                          persistent_workers=(NUM_WORKERS>0))\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(\"cuda\" in DEVICE),\n",
    "                          persistent_workers=(NUM_WORKERS>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subid = '421555'\n",
    "\n",
    "train_ds.packs[subid].shape\n",
    "\n",
    "los = 9\n",
    "flip = 0\n",
    "rot = 1\n",
    "i = los*8 + flip*4 + rot\n",
    "\n",
    "u = train_ds.packs[subid][i,4]\n",
    "v = train_ds.packs[subid][i,5]\n",
    "\n",
    "ny, nx = v.shape\n",
    "x = np.arange(nx)\n",
    "y = np.arange(ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "R = np.sqrt((X-W/2)**2 + (Y-H/2)**2)\n",
    "mask = np.array(R > 20)\n",
    "\n",
    "step=4\n",
    "# subsample\n",
    "sl = (slice(None, None, step), slice(None, None, step))\n",
    "Xs, Ys = X[sl], Y[sl]\n",
    "Vx, Vy = (mask*u)[sl], (mask*v)[sl]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.imshow(np.log10(train_ds.packs[subid][i,3]),origin='lower')\n",
    "q = ax.quiver(Xs,Ys,Vx,Vy,color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_ds.packs[subid][i,1],origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_ds[1150]['x'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.FPN(\n",
    "    encoder_name=\"resnet50\",       # good starting point; try \"resnet50\", \"convnext_tiny\", \"efficientnet-b3\", etc.\n",
    "    encoder_weights=\"imagenet\",    # <-- THIS loads pretrained encoder weights\n",
    "    in_channels=3,                 # your three velocity-bin brightness maps\n",
    "    classes=2,                     # 2 output channels (u, v)\n",
    "    activation=None                # regression: keep raw logits\n",
    ").to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scaler= torch.amp.GradScaler(enabled=(\"cuda\" in DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) train\n",
    "EPOCHS = 10\n",
    "FREEZE_ENCODER_EPOCHS = 0\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "#os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ckpt_path = os.path.join(\"checkpoints_test\", \"unet_cgm_best.pt\")\n",
    "\n",
    "for p in model.encoder.parameters(): p.requires_grad = False  # warmup 3–5 epochs\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if epoch > FREEZE_ENCODER_EPOCHS:\n",
    "        for p in model.encoder.parameters(): p.requires_grad = True\n",
    "            \n",
    "    tr_loss, tr_mae = train_one_epoch(model, train_loader, opt, scaler)\n",
    "    va_loss, va_mae = evaluate(model, test_loader)\n",
    "    print(f\"[{epoch:03d}/{EPOCHS}] train: loss {tr_loss:.5f}, mae {tr_mae:.5f} | \"\n",
    "          f\"val: loss {va_loss:.5f}, mae {va_mae:.5f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"mean\": mean, \"std\": std,\n",
    "            \"epoch\": epoch, \"val_loss\": va_loss,\n",
    "            \"config\": {\n",
    "                \"R_MASK\": R_MASK, \"H\": H, \"W\": W,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE, \"LR\": LR\n",
    "            }\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ✓ saved best → {ckpt_path}\")\n",
    "    # ----- save PERIODIC checkpoint every 10 epochs -----\n",
    "    if epoch % 10 == 0:\n",
    "        periodic_path = f\"checkpoints_test/unet_cgm_epoch{epoch:03d}.pt\"\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"mean\": mean, \"std\": std,\n",
    "            \"epoch\": epoch, \"val_loss\": va_loss,\n",
    "            \"config\": {\n",
    "                \"R_MASK\": R_MASK, \"H\": H, \"W\": W,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE, \"LR\": LR\n",
    "            }\n",
    "        }, periodic_path)\n",
    "        print(f\"  • saved periodic → {periodic_path}\")\n",
    "\n",
    "print(\"Done. Best val loss:\", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
