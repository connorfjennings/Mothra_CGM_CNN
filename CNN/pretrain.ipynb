{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, math, json, random, numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- config ----------------\n",
    "DATA_DIR   =  \"/home/cj535/palmer_scratch/TNG50_cutouts/MW_sample_maps/packed_aug8\"  # folder with your .npy packs\n",
    "PATTERN    = \"TNG50_snap099_subid*_views10_aug8_C5_256x256.npy\"\n",
    "H, W = 256, 256\n",
    "R_MASK = 20                     # pixels\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 2e-4\n",
    "NUM_WORKERS = 1\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VEL_BINS = [(-300, -100), (-100, 100), (100, 300)]  # 3 input channels\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- mask utilities ----------\n",
    "def circular_outer_mask(H: int, W: int, R: float, center=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns a mask of shape (1,H,W): 1 outside the circle of radius R, 0 inside.\n",
    "    center: (yc, xc) in pixel coords; default is image center.\n",
    "    \"\"\"\n",
    "    yc, xc = center if center is not None else (H/2.0, W/2.0)\n",
    "    yy, xx = torch.meshgrid(torch.arange(H, device=device),\n",
    "                            torch.arange(W, device=device), indexing=\"ij\")\n",
    "    rr2 = (yy - yc)**2 + (xx - xc)**2\n",
    "    mask = (rr2 >= R**2).float().unsqueeze(0)  # (1,H,W)\n",
    "    return mask\n",
    "\n",
    "# ---------- masked loss ----------\n",
    "def masked_mse(pred, target, mask, eps=1e-8):\n",
    "    \"\"\"\n",
    "    pred, target: (B,2,H,W)\n",
    "    mask: (B,1,H,W) with 1 where loss is computed (outside circle), 0 where ignored.\n",
    "    \"\"\"\n",
    "    diff2 = (pred - target)**2\n",
    "    wsum = mask.sum()\n",
    "    if wsum < eps:\n",
    "        # if everything is masked, return zero to avoid NaNs\n",
    "        return diff2.new_zeros(())\n",
    "    return (diff2 * mask).sum() / (wsum * pred.shape[1])  # average over valid pixels & channels\n",
    "\n",
    "def masked_mae(pred, target, mask, eps=1e-8):\n",
    "    diff = (pred - target).abs()\n",
    "    wsum = mask.sum()\n",
    "    if wsum < eps:\n",
    "        return diff.new_zeros(())\n",
    "    return (diff * mask).sum() / (wsum * pred.shape[1]).clamp_min(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subid_re = re.compile(r\".*?_subid(?P<subid>\\d+)_views10_aug8_C5_256x256\\.npy$\")\n",
    "def find_packs(data_dir: str) -> Dict[str, np.ndarray]:\n",
    "    packs = {}\n",
    "    for path in glob.glob(os.path.join(data_dir, PATTERN)):\n",
    "        m = subid_re.match(path)\n",
    "        if not m: \n",
    "            continue\n",
    "        subid = m.group(\"subid\")\n",
    "        # Load fully into RAM as float32 ndarray\n",
    "        arr = np.load(path)  # already float32 per your save; if not: .astype(np.float32, copy=False)\n",
    "        if arr.shape[1] != 5 or arr.shape[2:] != (H, W):\n",
    "            raise RuntimeError(f\"Unexpected shape {arr.shape} in {path}\")\n",
    "        packs[subid] = arr\n",
    "    if not packs:\n",
    "        raise RuntimeError(\"No packs found. Check DATA_DIR/PATTERN.\")\n",
    "    return packs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- dataset using in-RAM packs ----------------\n",
    "class GalaxyPackDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields individual (view×aug) samples from a list of subids.\n",
    "    Expects memory-resident dict: subid -> ndarray (N,5,H,W).\n",
    "    Normalization (mean/std) is applied to input channels only.\n",
    "    \"\"\"\n",
    "    def __init__(self, packs: Dict[str, np.ndarray], subids: List[str], mean=None, std=None, compression='sqrt',r_mask=R_MASK):\n",
    "        self.subids = list(subids)\n",
    "        self.packs = packs\n",
    "        # Build an index: for each subid, iterate over samples\n",
    "        self.items = []  # list of (subid, local_idx)\n",
    "        for sid in self.subids:\n",
    "            N = self.packs[sid].shape[0]\n",
    "            self.items.extend((sid, i) for i in range(N))\n",
    "        self.mask = circular_outer_mask(H, W, r_mask, device=\"cpu\")  # (1,H,W)\n",
    "        self.mean = mean  # (3,)\n",
    "        self.std  = std   # (3,)\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid, i = self.items[idx]\n",
    "        sample = self.packs[sid][i]       # (5,H,W) float32\n",
    "        x = sample[:3]                    # (3,H,W)\n",
    "        y = sample[3:]                    # (2,H,W)\n",
    "        if compression == 'sqrt':\n",
    "            x = np.sqrt(x)\n",
    "        elif compression == 'log10':\n",
    "            x = np.log10(x+1e-25)\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            x = (x - self.mean[:, None, None]) / (self.std[:, None, None] + 1e-21)\n",
    "        # zero inputs in center\n",
    "        x = x * self.mask.numpy()\n",
    "        return {\n",
    "            \"x\": torch.from_numpy(x),     # float32\n",
    "            \"y\": torch.from_numpy(y),\n",
    "            \"mask\": self.mask.clone(),    # torch float32 (1,H,W)\n",
    "            \"subid\": sid\n",
    "        }\n",
    "\n",
    "def compute_input_norm(packs: Dict[str, np.ndarray], subids: List[str],compression='sqrt'):\n",
    "    \"\"\"\n",
    "    Compute per-channel mean/std over the 3 brightness channels using ONLY the train subids.\n",
    "    \"\"\"\n",
    "    s = np.zeros(3, dtype=np.float64)\n",
    "    q = np.zeros(3, dtype=np.float64)\n",
    "    n = 0\n",
    "    for sid in subids:\n",
    "        arr = packs[sid]          # (N,5,H,W)\n",
    "        x = arr[:, :3, :, :]      # (N,3,H,W)\n",
    "        if compression == 'sqrt':\n",
    "            x = np.sqrt(x)\n",
    "        elif compression == 'log10':\n",
    "            x = np.log10(x+1e-25)\n",
    "        n += x.shape[0]*H*W\n",
    "        s += x.reshape(-1,3,H,W).transpose(1,0,2,3).reshape(3,-1).sum(axis=1)\n",
    "        q += (x**2).reshape(-1,3,H,W).transpose(1,0,2,3).reshape(3,-1).sum(axis=1)\n",
    "    mean = s / n\n",
    "    var  = (q / n) - mean**2\n",
    "    std  = np.sqrt(var)#np.sqrt(np.clip(var, 1e-12, None))\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "# ---------------- training / eval ----------------\n",
    "def train_one_epoch(model, loader, opt, scaler=None):\n",
    "    model.train()\n",
    "    tot_loss, tot_mae = 0.0, 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(DEVICE, non_blocking=True)\n",
    "        y = batch[\"y\"].to(DEVICE, non_blocking=True)\n",
    "        m = batch[\"mask\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = model(x)\n",
    "                loss = masked_mse(pred, y, m)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            pred = model(x); loss = masked_mse(pred, y, m)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "        mae = masked_mae(pred.detach(), y, m).item()\n",
    "        tot_loss += loss.item(); tot_mae += mae\n",
    "    n = len(loader)\n",
    "    return tot_loss/n, tot_mae/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    tot_loss, tot_mae = 0.0, 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(DEVICE, non_blocking=True)\n",
    "        y = batch[\"y\"].to(DEVICE, non_blocking=True)\n",
    "        m = batch[\"mask\"].to(DEVICE, non_blocking=True)\n",
    "        pred = model(x)\n",
    "        loss = masked_mse(pred, y, m)\n",
    "        mae  = masked_mae(pred, y, m).item()\n",
    "        tot_loss += loss.item(); tot_mae += mae\n",
    "    n = len(loader)\n",
    "    return tot_loss/n, tot_mae/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packs = find_packs(DATA_DIR)\n",
    "all_subids = sorted(packs.keys(), key=lambda s: int(s))\n",
    "print(f\"Loaded {len(all_subids)} galaxies into RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) split by subid (no leakage)\n",
    "groups = np.array([int(s) for s in all_subids])\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "# Split operates on indices; use subids as both samples and groups\n",
    "idx = np.arange(len(all_subids))\n",
    "train_idx, test_idx = next(splitter.split(idx, groups=groups))\n",
    "train_subids = [all_subids[i] for i in train_idx]\n",
    "test_subids  = [all_subids[i] for i in test_idx]\n",
    "print(f\"Train galaxies: {len(train_subids)} | Test galaxies: {len(test_subids)}\")\n",
    "\n",
    "\n",
    "compression = 'log10'\n",
    "# 3) compute input normalization on train only\n",
    "mean, std = compute_input_norm(packs, train_subids,compression=compression)\n",
    "#mean, std = np.array([0,0,0]), np.array([1,1,1])*1e-20\n",
    "print(\"Input mean:\", mean, \"std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "los = 9\n",
    "flip = 0\n",
    "rot = 0\n",
    "n = los*8 + flip*4 + rot\n",
    "Hmid = packs[train_subids[i]][n][1]\n",
    "if compression == 'log10':\n",
    "    Hmid = np.log10(Hmid)\n",
    "elif compression == 'sqrt':\n",
    "    Hmid = np.sqrt(Hmid)\n",
    "\n",
    "#Hmid = (Hmid - mean[1])# / std[1]\n",
    "plt.imshow(Hmid,origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "vel_u = packs[train_subids[i]][n][3]\n",
    "vel_v = packs[train_subids[i]][n][4]\n",
    "plt.imshow(vel_v,origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) datasets / loaders\n",
    "train_ds = GalaxyPackDataset(packs, train_subids, mean=mean, std=std, r_mask=R_MASK,compression=compression)\n",
    "test_ds  = GalaxyPackDataset(packs, test_subids,  mean=mean, std=std, r_mask=R_MASK,compression=compression)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(\"cuda\" in DEVICE),\n",
    "                          persistent_workers=(NUM_WORKERS>0))\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(\"cuda\" in DEVICE),\n",
    "                          persistent_workers=(NUM_WORKERS>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[1150]['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.FPN(\n",
    "    encoder_name=\"resnet50\",       # good starting point; try \"resnet50\", \"convnext_tiny\", \"efficientnet-b3\", etc.\n",
    "    encoder_weights=\"imagenet\",    # <-- THIS loads pretrained encoder weights\n",
    "    in_channels=3,                 # your three velocity-bin brightness maps\n",
    "    classes=2,                     # 2 output channels (u, v)\n",
    "    activation=None                # regression: keep raw logits\n",
    ").to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scaler= torch.amp.GradScaler(enabled=(\"cuda\" in DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) train\n",
    "EPOCHS = 10\n",
    "FREEZE_ENCODER_EPOCHS = 3\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "#os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ckpt_path = os.path.join(\"checkpoints_test\", \"unet_cgm_best.pt\")\n",
    "\n",
    "for p in model.encoder.parameters(): p.requires_grad = False  # warmup 3–5 epochs\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if epoch > FREEZE_ENCODER_EPOCHS:\n",
    "        for p in model.encoder.parameters(): p.requires_grad = True\n",
    "            \n",
    "    tr_loss, tr_mae = train_one_epoch(model, train_loader, opt, scaler)\n",
    "    va_loss, va_mae = evaluate(model, test_loader)\n",
    "    print(f\"[{epoch:03d}/{EPOCHS}] train: loss {tr_loss:.5f}, mae {tr_mae:.5f} | \"\n",
    "          f\"val: loss {va_loss:.5f}, mae {va_mae:.5f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"mean\": mean, \"std\": std,\n",
    "            \"epoch\": epoch, \"val_loss\": va_loss,\n",
    "            \"config\": {\n",
    "                \"R_MASK\": R_MASK, \"H\": H, \"W\": W,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE, \"LR\": LR\n",
    "            }\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ✓ saved best → {ckpt_path}\")\n",
    "    # ----- save PERIODIC checkpoint every 10 epochs -----\n",
    "    if epoch % 10 == 0:\n",
    "        periodic_path = f\"checkpoints_test/unet_cgm_epoch{epoch:03d}.pt\"\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"mean\": mean, \"std\": std,\n",
    "            \"epoch\": epoch, \"val_loss\": va_loss,\n",
    "            \"config\": {\n",
    "                \"R_MASK\": R_MASK, \"H\": H, \"W\": W,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE, \"LR\": LR\n",
    "            }\n",
    "        }, periodic_path)\n",
    "        print(f\"  • saved periodic → {periodic_path}\")\n",
    "\n",
    "print(\"Done. Best val loss:\", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
